{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14009614,"sourceType":"datasetVersion","datasetId":8924844}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Data Scraping\n- Reads TMDB_API_KEY from Kaggle Secrets\n- Saves raw JSON pages and a consolidated CSV under /kaggle/working\n- Filters movies between 2015 and 2025 with vote_average >= 6 and vote_count >= 50\n- Uses UTF-8 for all file writes","metadata":{}},{"cell_type":"code","source":"import time\nimport requests\nimport csv\nimport json\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient\n\n# === Get API key from Kaggle Secrets ===\nuser_secrets = UserSecretsClient()\nAPI_KEY = user_secrets.get_secret(\"TMDB_API_KEY\")\nif not API_KEY:\n    raise RuntimeError(\"TMDB_API_KEY not found in Kaggle Secrets. Add it in Notebook Settings.\")\n\n# === Output paths inside Kaggle working directory ===\nOUT_DIR = Path(\"/kaggle/working/data/raw/tmdb\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nCSV_PATH = OUT_DIR / \"movies_2015_2025.csv\"\n\n# === TMDB discover endpoint and base parameters ===\nBASE_URL = \"https://api.themoviedb.org/3/discover/movie\"\nparams = {\n    \"api_key\": API_KEY,\n    \"language\": \"en-US\",\n    \"sort_by\": \"popularity.desc\",\n    \"page\": 1,\n    \"include_adult\": False,\n    \"include_video\": False,\n    \"primary_release_date.gte\": \"2015-01-01\",\n    \"primary_release_date.lte\": \"2025-12-31\",\n    # Quality filters\n    \"vote_average.gte\": 6.0,\n    \"vote_count.gte\": 50,\n}\n\n# Safety / limits\nMAX_PAGES = 1000   # TMDB typically returns <= 500 pages, cap for safety\nREQUEST_DELAY = 0.35  # seconds between requests; increase if you see rate limits\n\ncollected = []\npage = 1\n\n# Simple retry wrapper for robustness\ndef safe_get(url, params, max_retries=3, timeout=30):\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = requests.get(url, params=params, timeout=timeout)\n            if resp.status_code == 200:\n                return resp\n            else:\n                print(f\"Request returned {resp.status_code}. Attempt {attempt}/{max_retries}.\")\n        except requests.RequestException as e:\n            print(f\"Request exception: {e}. Attempt {attempt}/{max_retries}.\")\n        time.sleep(1.0 * attempt)\n    return None\n\nprint(\"Starting TMDB fetch. Output directory:\", OUT_DIR)\n\nwhile True:\n    params[\"page\"] = page\n    resp = safe_get(BASE_URL, params)\n    if resp is None:\n        print(\"Failed to fetch page\", page, \"- stopping.\")\n        break\n\n    try:\n        data = resp.json()\n    except ValueError as e:\n        print(\"Failed to parse JSON for page\", page, e)\n        break\n\n    # Save raw JSON page as UTF-8\n    raw_path = OUT_DIR / f\"discover_page_{page}.json\"\n    raw_text = json.dumps(data, ensure_ascii=False)\n    raw_path.write_text(raw_text, encoding=\"utf-8\")\n\n    results = data.get(\"results\", [])\n    if not results:\n        print(\"No results on page\", page, \"-> stopping.\")\n        break\n\n    for m in results:\n        collected.append({\n            \"tmdb_id\": m.get(\"id\"),\n            \"title\": m.get(\"title\") or m.get(\"original_title\"),\n            \"release_date\": m.get(\"release_date\"),\n            \"year\": (m.get(\"release_date\") or \"\")[:4],\n            \"overview\": m.get(\"overview\"),\n            \"vote_average\": m.get(\"vote_average\"),\n            \"vote_count\": m.get(\"vote_count\"),\n            \"popularity\": m.get(\"popularity\"),\n            \"poster_path\": m.get(\"poster_path\")\n        })\n\n    print(f\"Saved page {page}, items {len(results)} (total collected: {len(collected)})\")\n\n    page += 1\n    total_pages = data.get(\"total_pages\", 0) or 0\n    if page > total_pages or page > MAX_PAGES:\n        print(f\"Reached last page or max pages (page {page}, total_pages {total_pages}).\")\n        break\n\n    time.sleep(REQUEST_DELAY)\n\n# Write consolidated CSV (UTF-8)\nfieldnames = [\"tmdb_id\",\"title\",\"release_date\",\"year\",\"overview\",\"vote_average\",\"vote_count\",\"popularity\",\"poster_path\"]\nwith CSV_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n    writer = csv.DictWriter(f, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in collected:\n        writer.writerow(row)\n\nprint(\"Done. Total movies collected:\", len(collected))\nprint(\"CSV saved to\", CSV_PATH)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:37:31.582751Z","iopub.execute_input":"2025-12-05T10:37:31.582983Z","iopub.status.idle":"2025-12-05T10:41:24.539904Z","shell.execute_reply.started":"2025-12-05T10:37:31.582957Z","shell.execute_reply":"2025-12-05T10:41:24.539272Z"}},"outputs":[{"name":"stdout","text":"Starting TMDB fetch. Output directory: /kaggle/working/data/raw/tmdb\nSaved page 1, items 20 (total collected: 20)\nSaved page 2, items 20 (total collected: 40)\nSaved page 3, items 20 (total collected: 60)\nSaved page 4, items 20 (total collected: 80)\nSaved page 5, items 20 (total collected: 100)\nSaved page 6, items 20 (total collected: 120)\nSaved page 7, items 20 (total collected: 140)\nSaved page 8, items 20 (total collected: 160)\nSaved page 9, items 20 (total collected: 180)\nSaved page 10, items 20 (total collected: 200)\nSaved page 11, items 20 (total collected: 220)\nSaved page 12, items 20 (total collected: 240)\nSaved page 13, items 20 (total collected: 260)\nSaved page 14, items 20 (total collected: 280)\nSaved page 15, items 20 (total collected: 300)\nSaved page 16, items 20 (total collected: 320)\nSaved page 17, items 20 (total collected: 340)\nSaved page 18, items 20 (total collected: 360)\nSaved page 19, items 20 (total collected: 380)\nSaved page 20, items 20 (total collected: 400)\nSaved page 21, items 20 (total collected: 420)\nSaved page 22, items 20 (total collected: 440)\nSaved page 23, items 20 (total collected: 460)\nSaved page 24, items 20 (total collected: 480)\nSaved page 25, items 20 (total collected: 500)\nSaved page 26, items 20 (total collected: 520)\nSaved page 27, items 20 (total collected: 540)\nSaved page 28, items 20 (total collected: 560)\nSaved page 29, items 20 (total collected: 580)\nSaved page 30, items 20 (total collected: 600)\nSaved page 31, items 20 (total collected: 620)\nSaved page 32, items 20 (total collected: 640)\nSaved page 33, items 20 (total collected: 660)\nSaved page 34, items 20 (total collected: 680)\nSaved page 35, items 20 (total collected: 700)\nSaved page 36, items 20 (total collected: 720)\nSaved page 37, items 20 (total collected: 740)\nSaved page 38, items 20 (total collected: 760)\nSaved page 39, items 20 (total collected: 780)\nSaved page 40, items 20 (total collected: 800)\nSaved page 41, items 20 (total collected: 820)\nSaved page 42, items 20 (total collected: 840)\nSaved page 43, items 20 (total collected: 860)\nSaved page 44, items 20 (total collected: 880)\nSaved page 45, items 20 (total collected: 900)\nSaved page 46, items 20 (total collected: 920)\nSaved page 47, items 20 (total collected: 940)\nSaved page 48, items 20 (total collected: 960)\nSaved page 49, items 20 (total collected: 980)\nSaved page 50, items 20 (total collected: 1000)\nSaved page 51, items 20 (total collected: 1020)\nSaved page 52, items 20 (total collected: 1040)\nSaved page 53, items 20 (total collected: 1060)\nSaved page 54, items 20 (total collected: 1080)\nSaved page 55, items 20 (total collected: 1100)\nSaved page 56, items 20 (total collected: 1120)\nSaved page 57, items 20 (total collected: 1140)\nSaved page 58, items 20 (total collected: 1160)\nSaved page 59, items 20 (total collected: 1180)\nSaved page 60, items 20 (total collected: 1200)\nSaved page 61, items 20 (total collected: 1220)\nSaved page 62, items 20 (total collected: 1240)\nSaved page 63, items 20 (total collected: 1260)\nSaved page 64, items 20 (total collected: 1280)\nSaved page 65, items 20 (total collected: 1300)\nSaved page 66, items 20 (total collected: 1320)\nSaved page 67, items 20 (total collected: 1340)\nSaved page 68, items 20 (total collected: 1360)\nSaved page 69, items 20 (total collected: 1380)\nSaved page 70, items 20 (total collected: 1400)\nSaved page 71, items 20 (total collected: 1420)\nSaved page 72, items 20 (total collected: 1440)\nSaved page 73, items 20 (total collected: 1460)\nSaved page 74, items 20 (total collected: 1480)\nSaved page 75, items 20 (total collected: 1500)\nSaved page 76, items 20 (total collected: 1520)\nSaved page 77, items 20 (total collected: 1540)\nSaved page 78, items 20 (total collected: 1560)\nSaved page 79, items 20 (total collected: 1580)\nSaved page 80, items 20 (total collected: 1600)\nSaved page 81, items 20 (total collected: 1620)\nSaved page 82, items 20 (total collected: 1640)\nSaved page 83, items 20 (total collected: 1660)\nSaved page 84, items 20 (total collected: 1680)\nSaved page 85, items 20 (total collected: 1700)\nSaved page 86, items 20 (total collected: 1720)\nSaved page 87, items 20 (total collected: 1740)\nSaved page 88, items 20 (total collected: 1760)\nSaved page 89, items 20 (total collected: 1780)\nSaved page 90, items 20 (total collected: 1800)\nSaved page 91, items 20 (total collected: 1820)\nSaved page 92, items 20 (total collected: 1840)\nSaved page 93, items 20 (total collected: 1860)\nSaved page 94, items 20 (total collected: 1880)\nSaved page 95, items 20 (total collected: 1900)\nSaved page 96, items 20 (total collected: 1920)\nSaved page 97, items 20 (total collected: 1940)\nSaved page 98, items 20 (total collected: 1960)\nSaved page 99, items 20 (total collected: 1980)\nSaved page 100, items 20 (total collected: 2000)\nSaved page 101, items 20 (total collected: 2020)\nSaved page 102, items 20 (total collected: 2040)\nSaved page 103, items 20 (total collected: 2060)\nSaved page 104, items 20 (total collected: 2080)\nSaved page 105, items 20 (total collected: 2100)\nSaved page 106, items 20 (total collected: 2120)\nSaved page 107, items 20 (total collected: 2140)\nSaved page 108, items 20 (total collected: 2160)\nSaved page 109, items 20 (total collected: 2180)\nSaved page 110, items 20 (total collected: 2200)\nSaved page 111, items 20 (total collected: 2220)\nSaved page 112, items 20 (total collected: 2240)\nSaved page 113, items 20 (total collected: 2260)\nSaved page 114, items 20 (total collected: 2280)\nSaved page 115, items 20 (total collected: 2300)\nSaved page 116, items 20 (total collected: 2320)\nSaved page 117, items 20 (total collected: 2340)\nSaved page 118, items 20 (total collected: 2360)\nSaved page 119, items 20 (total collected: 2380)\nSaved page 120, items 20 (total collected: 2400)\nSaved page 121, items 20 (total collected: 2420)\nSaved page 122, items 20 (total collected: 2440)\nSaved page 123, items 20 (total collected: 2460)\nSaved page 124, items 20 (total collected: 2480)\nSaved page 125, items 20 (total collected: 2500)\nSaved page 126, items 20 (total collected: 2520)\nSaved page 127, items 20 (total collected: 2540)\nSaved page 128, items 20 (total collected: 2560)\nSaved page 129, items 20 (total collected: 2580)\nSaved page 130, items 20 (total collected: 2600)\nSaved page 131, items 20 (total collected: 2620)\nSaved page 132, items 20 (total collected: 2640)\nSaved page 133, items 20 (total collected: 2660)\nSaved page 134, items 20 (total collected: 2680)\nSaved page 135, items 20 (total collected: 2700)\nSaved page 136, items 20 (total collected: 2720)\nSaved page 137, items 20 (total collected: 2740)\nSaved page 138, items 20 (total collected: 2760)\nSaved page 139, items 20 (total collected: 2780)\nSaved page 140, items 20 (total collected: 2800)\nSaved page 141, items 20 (total collected: 2820)\nSaved page 142, items 20 (total collected: 2840)\nSaved page 143, items 20 (total collected: 2860)\nSaved page 144, items 20 (total collected: 2880)\nSaved page 145, items 20 (total collected: 2900)\nSaved page 146, items 20 (total collected: 2920)\nSaved page 147, items 20 (total collected: 2940)\nSaved page 148, items 20 (total collected: 2960)\nSaved page 149, items 20 (total collected: 2980)\nSaved page 150, items 20 (total collected: 3000)\nSaved page 151, items 20 (total collected: 3020)\nSaved page 152, items 20 (total collected: 3040)\nSaved page 153, items 20 (total collected: 3060)\nSaved page 154, items 20 (total collected: 3080)\nSaved page 155, items 20 (total collected: 3100)\nSaved page 156, items 20 (total collected: 3120)\nSaved page 157, items 20 (total collected: 3140)\nSaved page 158, items 20 (total collected: 3160)\nSaved page 159, items 20 (total collected: 3180)\nSaved page 160, items 20 (total collected: 3200)\nSaved page 161, items 20 (total collected: 3220)\nSaved page 162, items 20 (total collected: 3240)\nSaved page 163, items 20 (total collected: 3260)\nSaved page 164, items 20 (total collected: 3280)\nSaved page 165, items 20 (total collected: 3300)\nSaved page 166, items 20 (total collected: 3320)\nSaved page 167, items 20 (total collected: 3340)\nSaved page 168, items 20 (total collected: 3360)\nSaved page 169, items 20 (total collected: 3380)\nSaved page 170, items 20 (total collected: 3400)\nSaved page 171, items 20 (total collected: 3420)\nSaved page 172, items 20 (total collected: 3440)\nSaved page 173, items 20 (total collected: 3460)\nSaved page 174, items 20 (total collected: 3480)\nSaved page 175, items 20 (total collected: 3500)\nSaved page 176, items 20 (total collected: 3520)\nSaved page 177, items 20 (total collected: 3540)\nSaved page 178, items 20 (total collected: 3560)\nSaved page 179, items 20 (total collected: 3580)\nSaved page 180, items 20 (total collected: 3600)\nSaved page 181, items 20 (total collected: 3620)\nSaved page 182, items 20 (total collected: 3640)\nSaved page 183, items 20 (total collected: 3660)\nSaved page 184, items 20 (total collected: 3680)\nSaved page 185, items 20 (total collected: 3700)\nSaved page 186, items 20 (total collected: 3720)\nSaved page 187, items 20 (total collected: 3740)\nSaved page 188, items 20 (total collected: 3760)\nSaved page 189, items 20 (total collected: 3780)\nSaved page 190, items 20 (total collected: 3800)\nSaved page 191, items 20 (total collected: 3820)\nSaved page 192, items 20 (total collected: 3840)\nSaved page 193, items 20 (total collected: 3860)\nSaved page 194, items 20 (total collected: 3880)\nSaved page 195, items 20 (total collected: 3900)\nSaved page 196, items 20 (total collected: 3920)\nSaved page 197, items 20 (total collected: 3940)\nSaved page 198, items 20 (total collected: 3960)\nSaved page 199, items 20 (total collected: 3980)\nSaved page 200, items 20 (total collected: 4000)\nSaved page 201, items 20 (total collected: 4020)\nSaved page 202, items 20 (total collected: 4040)\nSaved page 203, items 20 (total collected: 4060)\nSaved page 204, items 20 (total collected: 4080)\nSaved page 205, items 20 (total collected: 4100)\nSaved page 206, items 20 (total collected: 4120)\nSaved page 207, items 20 (total collected: 4140)\nSaved page 208, items 20 (total collected: 4160)\nSaved page 209, items 20 (total collected: 4180)\nSaved page 210, items 20 (total collected: 4200)\nSaved page 211, items 20 (total collected: 4220)\nSaved page 212, items 20 (total collected: 4240)\nSaved page 213, items 20 (total collected: 4260)\nSaved page 214, items 20 (total collected: 4280)\nSaved page 215, items 20 (total collected: 4300)\nSaved page 216, items 20 (total collected: 4320)\nSaved page 217, items 20 (total collected: 4340)\nSaved page 218, items 20 (total collected: 4360)\nSaved page 219, items 20 (total collected: 4380)\nSaved page 220, items 20 (total collected: 4400)\nSaved page 221, items 20 (total collected: 4420)\nSaved page 222, items 20 (total collected: 4440)\nSaved page 223, items 20 (total collected: 4460)\nSaved page 224, items 20 (total collected: 4480)\nSaved page 225, items 20 (total collected: 4500)\nSaved page 226, items 20 (total collected: 4520)\nSaved page 227, items 20 (total collected: 4540)\nSaved page 228, items 20 (total collected: 4560)\nSaved page 229, items 20 (total collected: 4580)\nSaved page 230, items 20 (total collected: 4600)\nSaved page 231, items 20 (total collected: 4620)\nSaved page 232, items 20 (total collected: 4640)\nSaved page 233, items 20 (total collected: 4660)\nSaved page 234, items 20 (total collected: 4680)\nSaved page 235, items 20 (total collected: 4700)\nSaved page 236, items 20 (total collected: 4720)\nSaved page 237, items 20 (total collected: 4740)\nSaved page 238, items 20 (total collected: 4760)\nSaved page 239, items 20 (total collected: 4780)\nSaved page 240, items 20 (total collected: 4800)\nSaved page 241, items 20 (total collected: 4820)\nSaved page 242, items 20 (total collected: 4840)\nSaved page 243, items 20 (total collected: 4860)\nSaved page 244, items 20 (total collected: 4880)\nSaved page 245, items 20 (total collected: 4900)\nSaved page 246, items 20 (total collected: 4920)\nSaved page 247, items 20 (total collected: 4940)\nSaved page 248, items 20 (total collected: 4960)\nSaved page 249, items 20 (total collected: 4980)\nSaved page 250, items 20 (total collected: 5000)\nSaved page 251, items 20 (total collected: 5020)\nSaved page 252, items 20 (total collected: 5040)\nSaved page 253, items 20 (total collected: 5060)\nSaved page 254, items 20 (total collected: 5080)\nSaved page 255, items 20 (total collected: 5100)\nSaved page 256, items 20 (total collected: 5120)\nSaved page 257, items 20 (total collected: 5140)\nSaved page 258, items 20 (total collected: 5160)\nSaved page 259, items 20 (total collected: 5180)\nSaved page 260, items 20 (total collected: 5200)\nSaved page 261, items 20 (total collected: 5220)\nSaved page 262, items 20 (total collected: 5240)\nSaved page 263, items 20 (total collected: 5260)\nSaved page 264, items 20 (total collected: 5280)\nSaved page 265, items 20 (total collected: 5300)\nSaved page 266, items 20 (total collected: 5320)\nSaved page 267, items 20 (total collected: 5340)\nSaved page 268, items 20 (total collected: 5360)\nSaved page 269, items 20 (total collected: 5380)\nSaved page 270, items 20 (total collected: 5400)\nSaved page 271, items 20 (total collected: 5420)\nSaved page 272, items 20 (total collected: 5440)\nSaved page 273, items 20 (total collected: 5460)\nSaved page 274, items 20 (total collected: 5480)\nSaved page 275, items 20 (total collected: 5500)\nSaved page 276, items 20 (total collected: 5520)\nSaved page 277, items 20 (total collected: 5540)\nSaved page 278, items 20 (total collected: 5560)\nSaved page 279, items 20 (total collected: 5580)\nSaved page 280, items 20 (total collected: 5600)\nSaved page 281, items 20 (total collected: 5620)\nSaved page 282, items 20 (total collected: 5640)\nSaved page 283, items 20 (total collected: 5660)\nSaved page 284, items 20 (total collected: 5680)\nSaved page 285, items 20 (total collected: 5700)\nSaved page 286, items 20 (total collected: 5720)\nSaved page 287, items 20 (total collected: 5740)\nSaved page 288, items 20 (total collected: 5760)\nSaved page 289, items 20 (total collected: 5780)\nSaved page 290, items 20 (total collected: 5800)\nSaved page 291, items 20 (total collected: 5820)\nSaved page 292, items 20 (total collected: 5840)\nSaved page 293, items 20 (total collected: 5860)\nSaved page 294, items 20 (total collected: 5880)\nSaved page 295, items 20 (total collected: 5900)\nSaved page 296, items 20 (total collected: 5920)\nSaved page 297, items 20 (total collected: 5940)\nSaved page 298, items 20 (total collected: 5960)\nSaved page 299, items 20 (total collected: 5980)\nSaved page 300, items 20 (total collected: 6000)\nSaved page 301, items 20 (total collected: 6020)\nSaved page 302, items 20 (total collected: 6040)\nSaved page 303, items 20 (total collected: 6060)\nSaved page 304, items 20 (total collected: 6080)\nSaved page 305, items 20 (total collected: 6100)\nSaved page 306, items 20 (total collected: 6120)\nSaved page 307, items 20 (total collected: 6140)\nSaved page 308, items 20 (total collected: 6160)\nSaved page 309, items 20 (total collected: 6180)\nSaved page 310, items 20 (total collected: 6200)\nSaved page 311, items 20 (total collected: 6220)\nSaved page 312, items 20 (total collected: 6240)\nSaved page 313, items 20 (total collected: 6260)\nSaved page 314, items 20 (total collected: 6280)\nSaved page 315, items 20 (total collected: 6300)\nSaved page 316, items 20 (total collected: 6320)\nSaved page 317, items 20 (total collected: 6340)\nSaved page 318, items 20 (total collected: 6360)\nSaved page 319, items 20 (total collected: 6380)\nSaved page 320, items 20 (total collected: 6400)\nSaved page 321, items 20 (total collected: 6420)\nSaved page 322, items 20 (total collected: 6440)\nSaved page 323, items 20 (total collected: 6460)\nSaved page 324, items 20 (total collected: 6480)\nSaved page 325, items 20 (total collected: 6500)\nSaved page 326, items 20 (total collected: 6520)\nSaved page 327, items 20 (total collected: 6540)\nSaved page 328, items 20 (total collected: 6560)\nSaved page 329, items 20 (total collected: 6580)\nSaved page 330, items 20 (total collected: 6600)\nSaved page 331, items 20 (total collected: 6620)\nSaved page 332, items 20 (total collected: 6640)\nSaved page 333, items 20 (total collected: 6660)\nSaved page 334, items 20 (total collected: 6680)\nSaved page 335, items 20 (total collected: 6700)\nSaved page 336, items 20 (total collected: 6720)\nSaved page 337, items 20 (total collected: 6740)\nSaved page 338, items 20 (total collected: 6760)\nSaved page 339, items 20 (total collected: 6780)\nSaved page 340, items 20 (total collected: 6800)\nSaved page 341, items 20 (total collected: 6820)\nSaved page 342, items 20 (total collected: 6840)\nSaved page 343, items 20 (total collected: 6860)\nSaved page 344, items 20 (total collected: 6880)\nSaved page 345, items 20 (total collected: 6900)\nSaved page 346, items 20 (total collected: 6920)\nSaved page 347, items 20 (total collected: 6940)\nSaved page 348, items 20 (total collected: 6960)\nSaved page 349, items 20 (total collected: 6980)\nSaved page 350, items 20 (total collected: 7000)\nSaved page 351, items 20 (total collected: 7020)\nSaved page 352, items 20 (total collected: 7040)\nSaved page 353, items 20 (total collected: 7060)\nSaved page 354, items 20 (total collected: 7080)\nSaved page 355, items 20 (total collected: 7100)\nSaved page 356, items 20 (total collected: 7120)\nSaved page 357, items 20 (total collected: 7140)\nSaved page 358, items 20 (total collected: 7160)\nSaved page 359, items 20 (total collected: 7180)\nSaved page 360, items 20 (total collected: 7200)\nSaved page 361, items 20 (total collected: 7220)\nSaved page 362, items 20 (total collected: 7240)\nSaved page 363, items 20 (total collected: 7260)\nSaved page 364, items 20 (total collected: 7280)\nSaved page 365, items 20 (total collected: 7300)\nSaved page 366, items 20 (total collected: 7320)\nSaved page 367, items 20 (total collected: 7340)\nSaved page 368, items 20 (total collected: 7360)\nSaved page 369, items 20 (total collected: 7380)\nSaved page 370, items 20 (total collected: 7400)\nSaved page 371, items 20 (total collected: 7420)\nSaved page 372, items 20 (total collected: 7440)\nSaved page 373, items 20 (total collected: 7460)\nSaved page 374, items 20 (total collected: 7480)\nSaved page 375, items 20 (total collected: 7500)\nSaved page 376, items 20 (total collected: 7520)\nSaved page 377, items 20 (total collected: 7540)\nSaved page 378, items 20 (total collected: 7560)\nSaved page 379, items 20 (total collected: 7580)\nSaved page 380, items 20 (total collected: 7600)\nSaved page 381, items 20 (total collected: 7620)\nSaved page 382, items 20 (total collected: 7640)\nSaved page 383, items 20 (total collected: 7660)\nSaved page 384, items 20 (total collected: 7680)\nSaved page 385, items 20 (total collected: 7700)\nSaved page 386, items 20 (total collected: 7720)\nSaved page 387, items 20 (total collected: 7740)\nSaved page 388, items 20 (total collected: 7760)\nSaved page 389, items 20 (total collected: 7780)\nSaved page 390, items 20 (total collected: 7800)\nSaved page 391, items 20 (total collected: 7820)\nSaved page 392, items 20 (total collected: 7840)\nSaved page 393, items 20 (total collected: 7860)\nSaved page 394, items 20 (total collected: 7880)\nSaved page 395, items 20 (total collected: 7900)\nSaved page 396, items 20 (total collected: 7920)\nSaved page 397, items 20 (total collected: 7940)\nSaved page 398, items 20 (total collected: 7960)\nSaved page 399, items 20 (total collected: 7980)\nSaved page 400, items 20 (total collected: 8000)\nSaved page 401, items 20 (total collected: 8020)\nSaved page 402, items 20 (total collected: 8040)\nSaved page 403, items 20 (total collected: 8060)\nSaved page 404, items 20 (total collected: 8080)\nSaved page 405, items 20 (total collected: 8100)\nSaved page 406, items 20 (total collected: 8120)\nSaved page 407, items 20 (total collected: 8140)\nSaved page 408, items 20 (total collected: 8160)\nSaved page 409, items 20 (total collected: 8180)\nSaved page 410, items 20 (total collected: 8200)\nSaved page 411, items 20 (total collected: 8220)\nSaved page 412, items 19 (total collected: 8239)\nReached last page or max pages (page 413, total_pages 412).\nDone. Total movies collected: 8239\nCSV saved to /kaggle/working/data/raw/tmdb/movies_2015_2025.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Step 2: TMDB enrichment","metadata":{}},{"cell_type":"code","source":"# tmdb_details_kaggle.py\n# Reads /kaggle/working/data/raw/tmdb/movies_2015_2025.csv\n# Fetches per-movie details: genres, credits (top 3 cast), keywords, external_ids (imdb_id), poster_url\n# Writes output to /kaggle/working/data/processed/movies_tmdb_enriched.csv\n\nimport time\nimport requests\nimport csv\nimport json\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nAPI_KEY = user_secrets.get_secret(\"TMDB_API_KEY\")\nif not API_KEY:\n    raise RuntimeError(\"TMDB_API_KEY not found in Kaggle Secrets.\")\n\nIN_PATH = Path(\"/kaggle/working/data/raw/tmdb/movies_2015_2025.csv\")\nOUT_DIR = Path(\"/kaggle/working/data/processed\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nOUT_CSV = OUT_DIR / \"movies_tmdb_enriched.csv\"\n\nBASE = \"https://api.themoviedb.org/3\"\nHEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; tmdb-enricher/1.0)\"}\n\ndef safe_get(url, params=None, retries=3):\n    for i in range(retries):\n        try:\n            r = requests.get(url, params=params, timeout=30, headers=HEADERS)\n            if r.status_code == 200:\n                return r.json()\n            else:\n                print(f\"TMDB request {r.status_code} for {url}\")\n        except Exception as e:\n            print(\"Request error:\", e)\n        time.sleep(1 + i)\n    return None\n\nrows = []\nwith IN_PATH.open(\"r\", encoding=\"utf-8\") as f:\n    reader = csv.DictReader(f)\n    for i,row in enumerate(reader):\n        tmdb_id = row.get(\"tmdb_id\")\n        if not tmdb_id:\n            continue\n        # movie details\n        details = safe_get(f\"{BASE}/movie/{tmdb_id}\", params={\"api_key\": API_KEY, \"language\": \"en-US\"})\n        # credits\n        credits = safe_get(f\"{BASE}/movie/{tmdb_id}/credits\", params={\"api_key\": API_KEY})\n        # keywords\n        keywords = safe_get(f\"{BASE}/movie/{tmdb_id}/keywords\", params={\"api_key\": API_KEY})\n        # external ids (to get imdb_id)\n        ext = safe_get(f\"{BASE}/movie/{tmdb_id}/external_ids\", params={\"api_key\": API_KEY})\n\n        genres_list = []\n        runtime = None\n        homepage = \"\"\n        if details:\n            genres_list = [g.get(\"name\") for g in details.get(\"genres\", [])] if details.get(\"genres\") else []\n            runtime = details.get(\"runtime\")\n            homepage = details.get(\"homepage\") or \"\"\n        cast_top3 = []\n        if credits:\n            cast = credits.get(\"cast\", [])[:10]\n            for c in cast[:3]:\n                name = c.get(\"name\")\n                character = c.get(\"character\")\n                cast_top3.append(name)\n        keyword_list = []\n        if keywords:\n            # TMDB keywords endpoint: {\"keywords\": [...] }\n            kw = keywords.get(\"keywords\") or keywords.get(\"results\") or []\n            keyword_list = [k.get(\"name\") for k in kw if k.get(\"name\")]\n        imdb_id = None\n        if ext:\n            imdb_id = ext.get(\"imdb_id\")\n        poster_path = row.get(\"poster_path\") or (details.get(\"poster_path\") if details else None)\n        poster_url = f\"https://image.tmdb.org/t/p/w500{poster_path}\" if poster_path else \"\"\n\n        new_row = {\n            \"tmdb_id\": tmdb_id,\n            \"title\": row.get(\"title\"),\n            \"release_date\": row.get(\"release_date\"),\n            \"year\": row.get(\"year\"),\n            \"overview\": (row.get(\"overview\") or \"\"),\n            \"vote_average\": row.get(\"vote_average\"),\n            \"vote_count\": row.get(\"vote_count\"),\n            \"popularity\": row.get(\"popularity\"),\n            \"genres\": \"|\".join(genres_list),\n            \"cast_top3\": \"|\".join(cast_top3),\n            \"keywords_tmdb\": \"|\".join(keyword_list),\n            \"imdb_id\": imdb_id or \"\",\n            \"poster_url\": poster_url,\n            \"runtime\": runtime,\n            \"homepage\": homepage\n        }\n        rows.append(new_row)\n        if (i+1) % 50 == 0:\n            print(f\"Processed {i+1} movies\")\n        time.sleep(0.25)  # be polite\n\n# Save CSV\nfieldnames = list(rows[0].keys()) if rows else [\"tmdb_id\",\"title\"]\nwith OUT_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n    writer = csv.DictWriter(f, fieldnames=fieldnames)\n    writer.writeheader()\n    for r in rows:\n        writer.writerow(r)\n\nprint(\"TMDB enrichment complete. Saved to\", OUT_CSV)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:09:08.736143Z","iopub.execute_input":"2025-12-05T11:09:08.736424Z","iopub.status.idle":"2025-12-05T13:10:13.337696Z","shell.execute_reply.started":"2025-12-05T11:09:08.736402Z","shell.execute_reply":"2025-12-05T13:10:13.336937Z"}},"outputs":[{"name":"stdout","text":"Processed 50 movies\nProcessed 100 movies\nProcessed 150 movies\nProcessed 200 movies\nProcessed 250 movies\nProcessed 300 movies\nProcessed 350 movies\nProcessed 400 movies\nProcessed 450 movies\nProcessed 500 movies\nProcessed 550 movies\nProcessed 600 movies\nProcessed 650 movies\nProcessed 700 movies\nProcessed 750 movies\nProcessed 800 movies\nProcessed 850 movies\nProcessed 900 movies\nProcessed 950 movies\nProcessed 1000 movies\nProcessed 1050 movies\nProcessed 1100 movies\nProcessed 1150 movies\nProcessed 1200 movies\nProcessed 1250 movies\nProcessed 1300 movies\nProcessed 1350 movies\nProcessed 1400 movies\nProcessed 1450 movies\nProcessed 1500 movies\nProcessed 1550 movies\nProcessed 1600 movies\nProcessed 1650 movies\nProcessed 1700 movies\nProcessed 1750 movies\nProcessed 1800 movies\nProcessed 1850 movies\nProcessed 1900 movies\nProcessed 1950 movies\nProcessed 2000 movies\nProcessed 2050 movies\nProcessed 2100 movies\nProcessed 2150 movies\nProcessed 2200 movies\nProcessed 2250 movies\nProcessed 2300 movies\nProcessed 2350 movies\nProcessed 2400 movies\nProcessed 2450 movies\nProcessed 2500 movies\nProcessed 2550 movies\nProcessed 2600 movies\nProcessed 2650 movies\nProcessed 2700 movies\nProcessed 2750 movies\nProcessed 2800 movies\nProcessed 2850 movies\nProcessed 2900 movies\nProcessed 2950 movies\nProcessed 3000 movies\nProcessed 3050 movies\nProcessed 3100 movies\nProcessed 3150 movies\nProcessed 3200 movies\nProcessed 3250 movies\nProcessed 3300 movies\nProcessed 3350 movies\nProcessed 3400 movies\nProcessed 3450 movies\nProcessed 3500 movies\nProcessed 3550 movies\nProcessed 3600 movies\nProcessed 3650 movies\nProcessed 3700 movies\nProcessed 3750 movies\nProcessed 3800 movies\nProcessed 3850 movies\nProcessed 3900 movies\nProcessed 3950 movies\nRequest error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nProcessed 4000 movies\nProcessed 4050 movies\nProcessed 4100 movies\nProcessed 4150 movies\nProcessed 4200 movies\nProcessed 4250 movies\nProcessed 4300 movies\nProcessed 4350 movies\nProcessed 4400 movies\nProcessed 4450 movies\nProcessed 4500 movies\nProcessed 4550 movies\nProcessed 4600 movies\nProcessed 4650 movies\nProcessed 4700 movies\nProcessed 4750 movies\nProcessed 4800 movies\nProcessed 4850 movies\nProcessed 4900 movies\nProcessed 4950 movies\nProcessed 5000 movies\nProcessed 5050 movies\nProcessed 5100 movies\nProcessed 5150 movies\nProcessed 5200 movies\nProcessed 5250 movies\nProcessed 5300 movies\nProcessed 5350 movies\nProcessed 5400 movies\nProcessed 5450 movies\nProcessed 5500 movies\nProcessed 5550 movies\nProcessed 5600 movies\nProcessed 5650 movies\nProcessed 5700 movies\nProcessed 5750 movies\nProcessed 5800 movies\nRequest error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nProcessed 5850 movies\nProcessed 5900 movies\nProcessed 5950 movies\nProcessed 6000 movies\nProcessed 6050 movies\nProcessed 6100 movies\nProcessed 6150 movies\nProcessed 6200 movies\nProcessed 6250 movies\nProcessed 6300 movies\nProcessed 6350 movies\nProcessed 6400 movies\nProcessed 6450 movies\nProcessed 6500 movies\nProcessed 6550 movies\nProcessed 6600 movies\nProcessed 6650 movies\nProcessed 6700 movies\nProcessed 6750 movies\nProcessed 6800 movies\nProcessed 6850 movies\nProcessed 6900 movies\nProcessed 6950 movies\nProcessed 7000 movies\nProcessed 7050 movies\nProcessed 7100 movies\nProcessed 7150 movies\nProcessed 7200 movies\nProcessed 7250 movies\nProcessed 7300 movies\nProcessed 7350 movies\nProcessed 7400 movies\nProcessed 7450 movies\nProcessed 7500 movies\nProcessed 7550 movies\nProcessed 7600 movies\nProcessed 7650 movies\nProcessed 7700 movies\nProcessed 7750 movies\nProcessed 7800 movies\nProcessed 7850 movies\nProcessed 7900 movies\nProcessed 7950 movies\nProcessed 8000 movies\nProcessed 8050 movies\nProcessed 8100 movies\nProcessed 8150 movies\nProcessed 8200 movies\nTMDB enrichment complete. Saved to /kaggle/working/data/processed/movies_tmdb_enriched.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# imdb_reviews_scraper","metadata":{}},{"cell_type":"code","source":"# imdb_reviews_scraper_kaggle.py\n# Reads movies_tmdb_enriched.csv, scrapes top 3 reviews from imdb for movies with imdb_id\n# Writes imdb_reviews.csv with columns: tmdb_id, imdb_id, title, review_idx, review_title, review_rating, review_text\n\nimport time\nimport csv\nfrom pathlib import Path\nfrom bs4 import BeautifulSoup\nimport requests\n\nIN_CSV = Path(\"/kaggle/working/data/processed/movies_tmdb_enriched.csv\")\nOUT_DIR = Path(\"/kaggle/working/data/processed/imdb_scraped\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nOUT_REV = OUT_DIR / \"imdb_reviews.csv\"\n\nHEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; imdb-scraper/1.0)\"}\n\ndef scrape_reviews(imdb_id, top_k=3):\n    url = f\"https://www.imdb.com/title/{imdb_id}/reviews\"\n    r = requests.get(url, headers=HEADERS, timeout=20)\n    if r.status_code != 200:\n        return []\n    soup = BeautifulSoup(r.text, \"html.parser\")\n    reviews = []\n    # IMDb structure: .review-container or .lister-item mode\n    containers = soup.select(\".review-container\") or soup.select(\".lister-item.mode-detail\")\n    for c in containers[:top_k]:\n        title_el = c.select_one(\".title\")\n        title = title_el.get_text(strip=True) if title_el else \"\"\n        content_el = c.select_one(\".text.show-more__control\")\n        if not content_el:\n            content_el = c.select_one(\".content .text\")\n        content = content_el.get_text(strip=True) if content_el else \"\"\n        rating_el = c.select_one(\".rating-other-user-rating span\")\n        rating = rating_el.get_text(strip=True) if rating_el else \"\"\n        reviews.append({\"title\": title, \"rating\": rating, \"content\": content})\n    # fallback simple parse\n    if not reviews:\n        # gather paragraphs\n        for p in soup.select(\"div.text\"):\n            text = p.get_text(strip=True)\n            if text:\n                reviews.append({\"title\": \"\", \"rating\": \"\", \"content\": text})\n                if len(reviews) >= top_k:\n                    break\n    return reviews\n\nwith IN_CSV.open(\"r\", encoding=\"utf-8\") as f_in, OUT_REV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n    reader = csv.DictReader(f_in)\n    writer = csv.writer(f_out)\n    writer.writerow([\"tmdb_id\",\"imdb_id\",\"title\",\"review_idx\",\"review_title\",\"review_rating\",\"review_text\"])\n    for i,row in enumerate(reader):\n        imdb_id = row.get(\"imdb_id\") or \"\"\n        tmdb_id = row.get(\"tmdb_id\")\n        title = row.get(\"title\")\n        if not imdb_id:\n            continue\n        try:\n            revs = scrape_reviews(imdb_id, top_k=3)\n            for idx,rv in enumerate(revs):\n                writer.writerow([tmdb_id, imdb_id, title, idx+1, rv.get(\"title\",\"\"), rv.get(\"rating\",\"\"), rv.get(\"content\",\"\")])\n        except Exception as e:\n            print(\"Error scraping\", imdb_id, e)\n        # polite delay; pause slightly more periodically\n        if (i+1) % 10 == 0:\n            time.sleep(2.0)\n        else:\n            time.sleep(0.6)\n        if (i+1) % 50 == 0:\n            print(\"Scraped reviews for\", i+1, \"movies\")\n\nprint(\"IMDb reviews scraping finished. Saved to\", OUT_REV)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:11:41.838477Z","iopub.execute_input":"2025-12-05T13:11:41.839156Z","iopub.status.idle":"2025-12-05T18:03:38.196831Z","shell.execute_reply.started":"2025-12-05T13:11:41.839127Z","shell.execute_reply":"2025-12-05T18:03:38.196134Z"}},"outputs":[{"name":"stdout","text":"Scraped reviews for 50 movies\nScraped reviews for 100 movies\nScraped reviews for 150 movies\nScraped reviews for 200 movies\nScraped reviews for 250 movies\nScraped reviews for 300 movies\nScraped reviews for 350 movies\nScraped reviews for 400 movies\nScraped reviews for 450 movies\nScraped reviews for 500 movies\nScraped reviews for 550 movies\nScraped reviews for 600 movies\nScraped reviews for 650 movies\nScraped reviews for 700 movies\nScraped reviews for 750 movies\nScraped reviews for 800 movies\nScraped reviews for 850 movies\nScraped reviews for 900 movies\nScraped reviews for 950 movies\nScraped reviews for 1000 movies\nScraped reviews for 1050 movies\nScraped reviews for 1100 movies\nScraped reviews for 1150 movies\nScraped reviews for 1200 movies\nScraped reviews for 1250 movies\nScraped reviews for 1300 movies\nScraped reviews for 1350 movies\nScraped reviews for 1400 movies\nScraped reviews for 1450 movies\nScraped reviews for 1500 movies\nScraped reviews for 1550 movies\nScraped reviews for 1600 movies\nScraped reviews for 1650 movies\nScraped reviews for 1700 movies\nScraped reviews for 1750 movies\nScraped reviews for 1800 movies\nScraped reviews for 1850 movies\nScraped reviews for 1900 movies\nScraped reviews for 1950 movies\nScraped reviews for 2000 movies\nScraped reviews for 2050 movies\nScraped reviews for 2100 movies\nScraped reviews for 2150 movies\nScraped reviews for 2200 movies\nScraped reviews for 2250 movies\nScraped reviews for 2300 movies\nScraped reviews for 2350 movies\nScraped reviews for 2400 movies\nScraped reviews for 2450 movies\nScraped reviews for 2500 movies\nScraped reviews for 2550 movies\nScraped reviews for 2600 movies\nScraped reviews for 2650 movies\nScraped reviews for 2700 movies\nScraped reviews for 2750 movies\nScraped reviews for 2800 movies\nScraped reviews for 2850 movies\nScraped reviews for 2900 movies\nScraped reviews for 2950 movies\nScraped reviews for 3000 movies\nScraped reviews for 3050 movies\nScraped reviews for 3100 movies\nScraped reviews for 3150 movies\nScraped reviews for 3200 movies\nError scraping tt31064841 ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nScraped reviews for 3250 movies\nScraped reviews for 3300 movies\nScraped reviews for 3350 movies\nScraped reviews for 3400 movies\nScraped reviews for 3450 movies\nScraped reviews for 3500 movies\nScraped reviews for 3550 movies\nScraped reviews for 3600 movies\nScraped reviews for 3650 movies\nScraped reviews for 3700 movies\nScraped reviews for 3750 movies\nScraped reviews for 3800 movies\nScraped reviews for 3850 movies\nScraped reviews for 3900 movies\nScraped reviews for 3950 movies\nScraped reviews for 4000 movies\nScraped reviews for 4050 movies\nScraped reviews for 4100 movies\nScraped reviews for 4150 movies\nScraped reviews for 4200 movies\nScraped reviews for 4250 movies\nScraped reviews for 4300 movies\nScraped reviews for 4350 movies\nScraped reviews for 4400 movies\nScraped reviews for 4450 movies\nScraped reviews for 4500 movies\nScraped reviews for 4550 movies\nScraped reviews for 4600 movies\nScraped reviews for 4650 movies\nScraped reviews for 4700 movies\nScraped reviews for 4750 movies\nScraped reviews for 4800 movies\nScraped reviews for 4850 movies\nScraped reviews for 4900 movies\nScraped reviews for 4950 movies\nScraped reviews for 5000 movies\nScraped reviews for 5050 movies\nScraped reviews for 5100 movies\nScraped reviews for 5150 movies\nScraped reviews for 5200 movies\nScraped reviews for 5250 movies\nScraped reviews for 5300 movies\nScraped reviews for 5350 movies\nScraped reviews for 5400 movies\nScraped reviews for 5450 movies\nScraped reviews for 5500 movies\nScraped reviews for 5550 movies\nScraped reviews for 5600 movies\nScraped reviews for 5650 movies\nScraped reviews for 5700 movies\nScraped reviews for 5750 movies\nScraped reviews for 5800 movies\nScraped reviews for 5850 movies\nScraped reviews for 5900 movies\nScraped reviews for 5950 movies\nScraped reviews for 6000 movies\nScraped reviews for 6050 movies\nScraped reviews for 6100 movies\nScraped reviews for 6150 movies\nScraped reviews for 6200 movies\nScraped reviews for 6250 movies\nScraped reviews for 6300 movies\nScraped reviews for 6350 movies\nScraped reviews for 6400 movies\nScraped reviews for 6450 movies\nScraped reviews for 6500 movies\nScraped reviews for 6550 movies\nScraped reviews for 6600 movies\nScraped reviews for 6650 movies\nScraped reviews for 6700 movies\nScraped reviews for 6750 movies\nScraped reviews for 6800 movies\nScraped reviews for 6850 movies\nScraped reviews for 6900 movies\nScraped reviews for 6950 movies\nScraped reviews for 7000 movies\nScraped reviews for 7050 movies\nScraped reviews for 7100 movies\nScraped reviews for 7150 movies\nScraped reviews for 7200 movies\nScraped reviews for 7250 movies\nScraped reviews for 7300 movies\nScraped reviews for 7350 movies\nScraped reviews for 7400 movies\nScraped reviews for 7450 movies\nScraped reviews for 7500 movies\nScraped reviews for 7550 movies\nScraped reviews for 7600 movies\nScraped reviews for 7650 movies\nScraped reviews for 7700 movies\nScraped reviews for 7750 movies\nScraped reviews for 7800 movies\nScraped reviews for 7850 movies\nScraped reviews for 7900 movies\nScraped reviews for 7950 movies\nScraped reviews for 8000 movies\nScraped reviews for 8050 movies\nScraped reviews for 8100 movies\nScraped reviews for 8150 movies\nScraped reviews for 8200 movies\nIMDb reviews scraping finished. Saved to /kaggle/working/data/processed/imdb_scraped/imdb_reviews.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# movielens_fetch_map","metadata":{}},{"cell_type":"code","source":"# movielens_fetch_map_kaggle.py\n# Downloads MovieLens ml-latest-small (or 20m) and maps to TMDB ids via links.csv.\n# Produces ratings_for_cf.csv containing userId,movieId(rating movieLens),tmdb_id,rating,timestamp\n\nimport os\nimport zipfile\nimport requests\nimport pandas as pd\nfrom pathlib import Path\n\nOUT_DIR = Path(\"/kaggle/working/data/ml\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n# choose dataset: 'ml-latest-small' (small) or 'ml-20m' (large if you want)\nML_NAME = \"ml-latest-small\"   # change to \"ml-20m\" if you need more\nML_URL = f\"https://files.grouplens.org/datasets/movielens/{ML_NAME}.zip\"\nZIP_PATH = OUT_DIR / f\"{ML_NAME}.zip\"\n\nprint(\"Downloading MovieLens:\", ML_URL)\nr = requests.get(ML_URL, stream=True)\nwith ZIP_PATH.open(\"wb\") as f:\n    for chunk in r.iter_content(chunk_size=8192):\n        if chunk:\n            f.write(chunk)\nprint(\"Downloaded to\", ZIP_PATH)\n\nwith zipfile.ZipFile(ZIP_PATH, 'r') as z:\n    z.extractall(OUT_DIR)\nprint(\"Extracted to\", OUT_DIR)\n\n# Read ratings and links\nratings = pd.read_csv(OUT_DIR / ML_NAME / \"ratings.csv\")\nlinks = pd.read_csv(OUT_DIR / ML_NAME / \"links.csv\")  # contains movieId, imdbId, tmdbId (may be NaN)\n\n# Read our processed TMDB enriched movie list to know which tmdb_ids exist\nmovies_tmdb = pd.read_csv(\"/kaggle/working/data/processed/movies_tmdb_enriched.csv\", dtype={\"tmdb_id\": str})\ntmdb_set = set(movies_tmdb[\"tmdb_id\"].astype(str).tolist())\n\n# Map MovieLens movieId -> tmdbId using links.csv, filter only those that have tmdbId and exist in our set\nlinks['tmdbId'] = links['tmdbId'].astype(pd.Int64Dtype()).astype(object)\nlinks['tmdbId'] = links['tmdbId'].apply(lambda x: str(int(x)) if pd.notna(x) else \"\")\nlinks_filtered = links[links['tmdbId'] != \"\"].copy()\nlinks_filtered = links_filtered[links_filtered['tmdbId'].isin(tmdb_set)]\n\nprint(\"MovieLens movies mapped to our TMDB list:\", links_filtered.shape[0])\n\n# Filter ratings to only these movieIds\nmovie_ids_to_keep = set(links_filtered['movieId'].tolist())\nratings_filtered = ratings[ratings['movieId'].isin(movie_ids_to_keep)].copy()\n\n# Merge to add tmdbId\nratings_filtered = ratings_filtered.merge(links_filtered[['movieId','tmdbId']], on='movieId', how='left')\nratings_filtered = ratings_filtered.rename(columns={'tmdbId': 'tmdb_id'})\n\n# Save ratings for collaborative filtering\nOUT_RAT = Path(\"/kaggle/working/data/processed/ratings_for_cf.csv\")\nratings_filtered.to_csv(OUT_RAT, index=False)\nprint(\"Saved filtered ratings to\", OUT_RAT)\nprint(\"Ratings count:\", len(ratings_filtered))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T18:03:38.198143Z","iopub.execute_input":"2025-12-05T18:03:38.198555Z","iopub.status.idle":"2025-12-05T18:03:38.959135Z","shell.execute_reply.started":"2025-12-05T18:03:38.198537Z","shell.execute_reply":"2025-12-05T18:03:38.958527Z"}},"outputs":[{"name":"stdout","text":"Downloading MovieLens: https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\nDownloaded to /kaggle/working/data/ml/ml-latest-small.zip\nExtracted to /kaggle/working/data/ml\nMovieLens movies mapped to our TMDB list: 478\nSaved filtered ratings to /kaggle/working/data/processed/ratings_for_cf.csv\nRatings count: 2060\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# merge_master_dataset","metadata":{}},{"cell_type":"code","source":"# merge_master_dataset_fixed.py\n# Robust merge script that handles NaNs and flexible paths (Kaggle).\n# Produces movies_master.csv with 'combined_text' column.\n\nimport pandas as pd\nfrom pathlib import Path\nimport json\n\n# Helper: find a file by name under possible base dirs\ndef find_file(filename, candidates=None):\n    if candidates is None:\n        candidates = [\n            Path(\"/kaggle/working/data/processed\"),\n            Path(\"/kaggle/working/data/raw/tmdb\"),\n            Path(\"/kaggle/input/high-rated-movies-dataset-20152025\"),\n            Path(\"/kaggle/input\"),\n            Path(\".\"),\n        ]\n    for base in candidates:\n        p = base / filename\n        if p.exists():\n            return p\n    # try a recursive search (limited)\n    for base in candidates:\n        if base.exists():\n            for found in base.rglob(filename):\n                return found\n    return None\n\n# locate TMDB enriched file\ntmdb_path = find_file(\"movies_tmdb_enriched.csv\")\nif not tmdb_path:\n    tmdb_path = find_file(\"movies_2015_2025.csv\")  # fallback to raw TMDB CSV if enriched missing\n    if tmdb_path:\n        print(\"Found raw TMDB CSV at\", tmdb_path, \"- it may lack genres/keywords/credits.\")\nif not tmdb_path:\n    raise FileNotFoundError(\"Could not find movies_tmdb_enriched.csv or movies_2015_2025.csv. Place it under /kaggle/working/data/processed or /kaggle/input.\")\n\nprint(\"Loading movies from:\", tmdb_path)\nmovies_tmdb = pd.read_csv(tmdb_path, dtype=str).fillna(\"\")  # read everything as str, fill NaN with empty string\n\n# attempt to find imdb reviews file\nrev_path = find_file(\"imdb_reviews.csv\", candidates=[Path(\"/kaggle/working/data/processed/imdb_scraped\"), Path(\"/kaggle/working/data/processed\"), Path(\"/kaggle/input\")])\nif rev_path and rev_path.exists():\n    print(\"Loading scraped reviews from:\", rev_path)\n    reviews = pd.read_csv(rev_path, dtype=str).fillna(\"\")\n    # combine reviews per movie into single text field\n    reviews_grouped = reviews.groupby('tmdb_id')['review_text'].apply(lambda texts: \" \".join([t for t in texts if str(t).strip() != \"\"])).reset_index()\n    movies = movies_tmdb.merge(reviews_grouped, left_on='tmdb_id', right_on='tmdb_id', how='left')\n    movies['review_text'] = movies['review_text'].fillna(\"\")\nelse:\n    print(\"No imdb_reviews.csv found. Continuing without review_text.\")\n    movies = movies_tmdb.copy()\n    movies['review_text'] = \"\"\n\n# Create robust combined_text from multiple columns; skip empty strings\ndef safe_join(parts):\n    out = []\n    for p in parts:\n        if p is None:\n            continue\n        s = str(p)\n        if s.strip() == \"\" or s.strip().lower() == \"nan\":\n            continue\n        out.append(s.strip())\n    return \" . \".join(out)\n\n# Normalize some columns if they exist\nfor col in ['genres','keywords_tmdb','cast_top3','overview','title','review_text']:\n    if col not in movies.columns:\n        movies[col] = \"\"\n\n# Build combined_text\nmovies['combined_text'] = movies.apply(lambda r: safe_join([\n    r.get('title',''),\n    r.get('overview',''),\n    r.get('genres','').replace(\"|\",\" \") if isinstance(r.get('genres',''), str) else str(r.get('genres','')),\n    r.get('keywords_tmdb','').replace(\"|\",\" \") if isinstance(r.get('keywords_tmdb',''), str) else str(r.get('keywords_tmdb','')),\n    r.get('cast_top3','').replace(\"|\",\" \") if isinstance(r.get('cast_top3',''), str) else str(r.get('cast_top3','')),\n    r.get('review_text','')\n]), axis=1)\n\n# some optional cleanup: ensure year numeric where possible\nif 'year' in movies.columns:\n    movies['year'] = pd.to_numeric(movies['year'], errors='coerce')\n\nOUT_DIR = Path(\"/kaggle/working/data/processed\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nOUT_MOV = OUT_DIR / \"movies_master.csv\"\nmovies.to_csv(OUT_MOV, index=False, encoding=\"utf-8\")\nprint(\"Saved movies_master to\", OUT_MOV)\nprint(\"movies_master shape:\", movies.shape)\n\n# quick sanity print: show first 3 combined_text samples\nfor i, txt in enumerate(movies['combined_text'].head(3)):\n    print(f\"\\n--- sample {i+1} ---\\n{txt[:800]}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:25:48.628532Z","iopub.execute_input":"2025-12-10T16:25:48.628832Z","iopub.status.idle":"2025-12-10T16:25:49.379317Z","shell.execute_reply.started":"2025-12-10T16:25:48.628810Z","shell.execute_reply":"2025-12-10T16:25:49.378671Z"}},"outputs":[{"name":"stdout","text":"Found raw TMDB CSV at /kaggle/input/high-rated-movies-dataset-20152025/movies_2015_2025.csv - it may lack genres/keywords/credits.\nLoading movies from: /kaggle/input/high-rated-movies-dataset-20152025/movies_2015_2025.csv\nNo imdb_reviews.csv found. Continuing without review_text.\nSaved movies_master to /kaggle/working/data/processed/movies_master.csv\nmovies_master shape: (8239, 14)\n\n--- sample 1 ---\nZootopia 2 . After cracking the biggest case in Zootopia's history, rookie cops Judy Hopps and Nick Wilde find themselves on the twisting trail of a great mystery when Gary DeSnake arrives and turns the animal metropolis upside down. To crack the case, Judy and Nick must go undercover to unexpected new parts of town, where their growing partnership is tested like never before.\n\n\n--- sample 2 ---\nTRON: Ares . A highly sophisticated Program called Ares is sent from the digital world into the real world on a dangerous mission, marking humankind's first encounter with A.I. beings.\n\n\n--- sample 3 ---\nTroll 2 . When a dangerous new troll unleashes devastation across their homeland, Nora, Andreas and Major Kris embark on their most perilous mission yet.\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}